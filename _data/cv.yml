cv:
  name: Xiuqi Zhu
  label: MS in Information Science @ CMU
  email: xiuqizhu@andrew.cmu.edu
  location: Pittsburgh, PA / Remote
  image: assets/img/richard.jpg
  summary: Master's student at CMU. Focus on deep learning, generative AI, and efficient LLM systems. Interned at Qunar and Shanghai Petroleum & Natural Gas Exchange.

  social_networks:
    - network: GitHub
      username: xiuqi-zhu
    - network: LinkedIn
      username: xiuqizhu

  address:
    street: ""
    city: Pittsburgh
    region: PA
    postalCode: ""
    countryCode: US

  sections:
    Education:
      - institution: Carnegie Mellon University
        location: Pittsburgh, PA, USA
        url: https://www.cmu.edu/
        area: Information Science
        studyType: MS
        start_date: "2025-08"
        end_date: "2026-12"
        score: "3.7/4.0"
        courses: "Deep Learning, Generative AI, LLM Systems, Deep RL, NLP"
        highlights: []

      - institution: Central University of Finance and Economics
        location: Beijing, China
        url: https://en.cufe.edu.cn/
        area: Data Science
        studyType: B.S.
        start_date: "2021-09"
        end_date: "2025-06"
        score: "3.8/4.0"
        courses: "Data Structures, Machine Learning, Statistical Modeling, Distributed Big Data, Database Systems"
        highlights: []

    Experience:
      - company: Carnegie Mellon University
        position: Research Assistant (Advisor: Prof. Virginia Smith)
        location: Pittsburgh, PA
        start_date: "2025-08"
        end_date: Present
        summary: Dynamic LoRA for efficient LLM training (target NeurIPS 2026).
        highlights:
          - "DFLoRA: gradient-norm–based switching between LoRA ranks and full fine-tuning; +2.1% benchmark score vs static LoRA."
          - "MoE-inspired parameter routing to preserve optimizer momentum; 42% less catastrophic forgetting vs full fine-tuning."
          - "Training pipeline on Qwen2.5 and Gemma 3 across 10+ tasks."

      - company: Qunar (去哪儿旅行)
        position: Algorithm Intern (User Growth)
        location: Beijing, China
        start_date: "2024-10"
        end_date: "2025-03"
        summary: Multi-agent RAG pipelines and multi-modal prediction for travel ads.
        highlights:
          - "LangGraph multi-agent system for search-to-ad copy; RAG over travel KB; 60% throughput gain."
          - "MMoE multi-modal model (ViT, BERT, CLIP) for CTR/like/conversion; +8% CTR, +2% ROI in A/B tests."
          - "300k+ query–POI/city pipeline with text2vec; 97.5% retention-set accuracy."

      - company: Shanghai Petroleum and Natural Gas Exchange
        position: Machine Learning Algorithm Intern
        location: Shanghai, China
        start_date: "2024-07"
        end_date: "2024-09"
        summary: ETL pipelines and volatility-aware pricing models.
        highlights:
          - "API-based ETL (Aliyun → Hadoop); 10k+ records/day; 35% shorter E2E time."
          - "XGBoost pricing model with market indices and time-decay weighting; MAPE −8%."

    Volunteer: []

    Awards: []

    Publications:
      - title: Multimodal Emotion Recognition Improvement Based on Stacking Ensemble Learning
        authors:
          - "Xiuqi Zhu"
        publisher: ADMA 2025
        releaseDate: "2026"
        url: "#"
        summary: Stacking ensemble for multimodal emotion recognition.

    Skills:
      - name: Programming
        level: Proficient
        icon: fa-solid fa-code
        keywords: "Python, C++, SQL, Java, Scala, R, Bash"

      - name: AI & ML
        level: Proficient
        icon: fa-solid fa-brain
        keywords: "PyTorch, TensorFlow, Scikit-learn, Hugging Face, LangChain, RAG, Deep Learning"

      - name: Cloud & Tools
        level: Familiar
        icon: fa-solid fa-server
        keywords: "Docker, Git, Kafka, Spark, Hadoop, AWS, Kubernetes, FastAPI"

    Languages:
      - name: Chinese
        summary: "Native"
      - name: English
        summary: "Fluent"

    Interests:
      - name: AI Systems
        icon: fa-solid fa-tag
        keywords: "LLM fine-tuning, multi-agent systems, speech recognition"

    Certificates: []

    Projects:
      - name: Dynamic LoRA for Efficient LLM Training
        summary: Gradient-norm–based dynamic switching; MoE-inspired routing. Target NeurIPS 2026.
        highlights:
          - "DFLoRA framework"
          - "Qwen2.5 & Gemma 3 benchmarks"
        start_date: "2025-08"
        end_date: Present

      - name: Transformer-based Speech Recognition (Kaggle Top 10%)
        summary: Progressive training, CTC+CE, shallow fusion. 10.5% CER on test set.
        highlights:
          - "Progressive training"
          - "10.5% CER"
        start_date: "2025-09"
        end_date: "2025-12"

    References: []
